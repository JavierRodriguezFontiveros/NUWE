{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Index(['Measurement date', 'Station code', 'Latitude', 'Longitude', 'SO2',\n",
    "       'NO2', 'O3', 'CO', 'PM10', 'PM2.5'],\n",
    "      dtype='object') dataset: data\n",
    "Index(['Measurement date', 'Station code', 'Latitude', 'Longitude', 'SO2',\n",
    "       'NO2', 'O3', 'CO', 'PM10', 'PM2.5'],\n",
    "      dtype='object') dataset: instrument\n",
    "Index(['Measurement date', 'Station code', 'Latitude', 'Longitude', 'SO2',\n",
    "       'NO2', 'O3', 'CO', 'PM10', 'PM2.5'],\n",
    "      dtype='object') dataset: pollutant```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 92/550 puntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrument status\n",
      "0    97.381889\n",
      "8     0.764730\n",
      "1     0.732977\n",
      "9     0.531042\n",
      "4     0.484925\n",
      "2     0.104437\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Procesando estación 205 con contaminante SO2\n",
      "Clasificación de anomalías - F1 Score: 0.6665\n",
      "\n",
      "Procesando estación 209 con contaminante NO2\n",
      "Clasificación de anomalías - F1 Score: 0.6869\n",
      "\n",
      "Procesando estación 223 con contaminante O3\n",
      "Clasificación de anomalías - F1 Score: 0.4489\n",
      "\n",
      "Procesando estación 224 con contaminante CO\n",
      "Clasificación de anomalías - F1 Score: 0.4498\n",
      "\n",
      "Procesando estación 226 con contaminante PM10\n",
      "Clasificación de anomalías - F1 Score: 0.5407\n",
      "\n",
      "Procesando estación 227 con contaminante PM2.5\n",
      "Clasificación de anomalías - F1 Score: 0.5620\n",
      "Predicciones guardadas en predictions/predictions_task_3.json\n",
      "Instrument status\n",
      "0    97.381889\n",
      "8     0.764730\n",
      "1     0.732977\n",
      "9     0.531042\n",
      "4     0.484925\n",
      "2     0.104437\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv(\"data/raw/measurement_data.csv\")\n",
    "instrument_data = pd.read_csv(\"data/raw/instrument_data.csv\", parse_dates=['Measurement date'])\n",
    "\n",
    "print(instrument_data[\"Instrument status\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Convertir fechas a tipo datetime\n",
    "data['Measurement date'] = pd.to_datetime(data['Measurement date'])\n",
    "instrument_data['Measurement date'] = pd.to_datetime(instrument_data['Measurement date'])\n",
    "\n",
    "# Realizar el merge entre data e instrument_data\n",
    "merged_data = pd.merge(data, instrument_data, on=['Station code', 'Measurement date'], how='inner')\n",
    "\n",
    "# Extraer características temporales\n",
    "merged_data[\"hour\"] = merged_data[\"Measurement date\"].dt.hour\n",
    "merged_data[\"month\"] = merged_data[\"Measurement date\"].dt.month\n",
    "merged_data[\"weekday\"] = merged_data[\"Measurement date\"].dt.weekday\n",
    "merged_data[\"is_weekend\"] = merged_data[\"weekday\"].isin([5, 6]).astype(int)\n",
    "\n",
    "# Crear nuevas características\n",
    "# Promedio móvil de 3 horas para los contaminantes\n",
    "for pollutant in ['SO2', 'NO2', 'O3', 'CO', 'PM10', 'PM2.5']:\n",
    "    merged_data[f'{pollutant}_rolling_mean'] = merged_data[pollutant].rolling(window=3).mean()\n",
    "\n",
    "# Relación entre contaminantes\n",
    "merged_data['SO2_NO2'] = merged_data['SO2'] / (merged_data['NO2'] + 1e-5)  # Agregar un pequeño valor para evitar división por 0\n",
    "merged_data['O3_PM10'] = merged_data['O3'] / (merged_data['PM10'] + 1e-5)\n",
    "merged_data['SO2_O3'] = merged_data['SO2'] / (merged_data['O3'] + 1e-5)\n",
    "\n",
    "# Clasificar el `Instrument status` en binario (0 -> Normal, 1 -> Anómalo)\n",
    "merged_data['instrument_status_binary'] = np.where(merged_data['Instrument status'] == 0, 0, 1)\n",
    "\n",
    "\n",
    "# Definir las estaciones y contaminantes con sus períodos\n",
    "stations = {\n",
    "    \"205\": (\"SO2\", '2023-11-01 00:00:00', '2023-11-30 23:00:00'),\n",
    "    \"209\": (\"NO2\", '2023-09-01 00:00:00', '2023-09-30 23:00:00'),\n",
    "    \"223\": (\"O3\", '2023-07-01 00:00:00', '2023-07-31 23:00:00'),\n",
    "    \"224\": (\"CO\", '2023-10-01 00:00:00', '2023-10-31 23:00:00'),\n",
    "    \"226\": (\"PM10\", '2023-08-01 00:00:00', '2023-08-31 23:00:00'),\n",
    "    \"227\": (\"PM2.5\", '2023-12-01 00:00:00', '2023-12-31 23:00:00')\n",
    "}\n",
    "\n",
    "# Inicializar el diccionario de resultados\n",
    "output = {\"target\": {}}\n",
    "\n",
    "for station_code, (pollutant, start_date, end_date) in stations.items():\n",
    "    print(f\"\\nProcesando estación {station_code} con contaminante {pollutant}\")\n",
    "    station_data = merged_data[(merged_data['Station code'] == int(station_code)) & (merged_data[pollutant] >= 0)].copy()\n",
    "    \n",
    "    if station_data.empty:\n",
    "        continue\n",
    "    \n",
    "    # Aplicar Isolation Forest para detectar anomalías\n",
    "    features = ['hour', 'weekday', 'month', pollutant]\n",
    "    iso_forest = IsolationForest(contamination=0.03, random_state=42)\n",
    "    station_data['anomaly'] = iso_forest.fit_predict(station_data[features])\n",
    "    station_data['anomaly'] = np.where(station_data['anomaly'] == -1, 1, 0)\n",
    "    \n",
    "    # Filtrar anomalías detectadas\n",
    "    anomalies = station_data[station_data['anomaly'] == 1].copy()\n",
    "    if anomalies.empty:\n",
    "        continue\n",
    "    \n",
    "    # Entrenar modelo multiclase para clasificar anomalías\n",
    "    X_anomalies = anomalies[features]\n",
    "    y_anomalies = anomalies['Instrument status']\n",
    "    if y_anomalies.isnull().all():\n",
    "        continue\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_anomalies, y_anomalies, test_size=0.2, random_state=42)\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Clasificación de anomalías - F1 Score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "    \n",
    "    # Predecir anomalías en el período solicitado\n",
    "    date_range = pd.date_range(start=pd.to_datetime(start_date), end=pd.to_datetime(end_date), freq='h')\n",
    "    forecast_data = pd.DataFrame({'Measurement date': date_range})\n",
    "    forecast_data['hour'] = forecast_data['Measurement date'].dt.hour\n",
    "    forecast_data['weekday'] = forecast_data['Measurement date'].dt.weekday\n",
    "    forecast_data['month'] = forecast_data['Measurement date'].dt.month\n",
    "    forecast_data[pollutant] = 0  # Se podría usar un valor estimado o promedio\n",
    "    \n",
    "    # Aplicar Isolation Forest en los datos futuros\n",
    "    forecast_data['anomaly'] = iso_forest.predict(forecast_data[features])\n",
    "    forecast_data['anomaly'] = np.where(forecast_data['anomaly'] == -1, 1, 0)\n",
    "    \n",
    "    anomalous_data = forecast_data[forecast_data['anomaly'] == 1].copy()\n",
    "    if not anomalous_data.empty:\n",
    "        anomalous_data['anomaly_type'] = clf.predict(anomalous_data[features])\n",
    "    \n",
    "    final_results = forecast_data.merge(anomalous_data[['Measurement date', 'anomaly_type']], on='Measurement date', how='left')\n",
    "    output[\"target\"][station_code] = {str(date): int(pred) if not np.isnan(pred) else 0 for date, pred in zip(final_results['Measurement date'], final_results['anomaly_type'].fillna(0))}\n",
    "\n",
    "# Guardar predicciones\n",
    "output_filename = \"predictions/predictions_task_3.json\"\n",
    "os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "print(f\"Predicciones guardadas en {output_filename}\")\n",
    "print(instrument_data[\"Instrument status\"].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Station code      Latitude     Longitude           SO2           NO2  \\\n",
      "count       24453.0  2.445300e+04  2.445300e+04  24453.000000  24453.000000   \n",
      "mean          205.0  3.756426e+01  1.269747e+02      0.003272      0.031749   \n",
      "std             0.0  7.105573e-15  1.421115e-14      0.018240      0.025728   \n",
      "min           205.0  3.756426e+01  1.269747e+02     -1.000000     -1.000000   \n",
      "25%           205.0  3.756426e+01  1.269747e+02      0.003000      0.019000   \n",
      "50%           205.0  3.756426e+01  1.269747e+02      0.003000      0.029000   \n",
      "75%           205.0  3.756426e+01  1.269747e+02      0.004000      0.043000   \n",
      "max           205.0  3.756426e+01  1.269747e+02      0.082000      0.248000   \n",
      "\n",
      "                 O3            CO          PM10         PM2.5     Item code  \\\n",
      "count  24453.000000  24453.000000  24453.000000  24453.000000  24453.000000   \n",
      "mean       0.024997      0.496716     37.973459     22.708502      1.518219   \n",
      "std        0.042638      0.256208     25.970254     17.344696      2.937438   \n",
      "min       -1.000000     -1.000000     -1.000000     -1.000000      0.000000   \n",
      "25%        0.009000      0.300000     21.000000     11.000000      0.000000   \n",
      "50%        0.023000      0.400000     32.000000     18.000000      0.000000   \n",
      "75%        0.037000      0.600000     48.000000     29.000000      0.000000   \n",
      "max        0.178000      8.700000    296.000000    276.000000      8.000000   \n",
      "\n",
      "       ...  SO2_rolling_mean  NO2_rolling_mean  O3_rolling_mean  \\\n",
      "count  ...      24453.000000      24453.000000     24453.000000   \n",
      "mean   ...          0.003272          0.031748         0.024996   \n",
      "std    ...          0.014284          0.022297         0.039973   \n",
      "min    ...         -1.000000         -1.000000        -1.000000   \n",
      "25%    ...          0.002667          0.019667         0.009667   \n",
      "50%    ...          0.003000          0.029333         0.023000   \n",
      "75%    ...          0.004000          0.042333         0.037333   \n",
      "max    ...          0.063667          0.178000         0.174333   \n",
      "\n",
      "       CO_rolling_mean  PM10_rolling_mean  PM2.5_rolling_mean       SO2_NO2  \\\n",
      "count     24453.000000       24453.000000        24453.000000  24453.000000   \n",
      "mean          0.496713          37.970133           22.707820      0.158981   \n",
      "std           0.244196          25.632399           17.072131      2.125589   \n",
      "min          -1.000000          -1.000000           -1.000000     -0.001000   \n",
      "25%           0.333333          21.333333           11.666667      0.081059   \n",
      "50%           0.433333          32.000000           18.333333      0.111070   \n",
      "75%           0.600000          48.000000           28.666667      0.166597   \n",
      "max           6.733333         287.000000          190.333333    300.000000   \n",
      "\n",
      "            O3_PM10        SO2_O3  instrument_status_binary  \n",
      "count  24453.000000  24453.000000              24453.000000  \n",
      "mean       1.420375      0.477596                  0.007116  \n",
      "std       82.765832      3.269270                  0.084056  \n",
      "min       -0.111111     -0.003000                  0.000000  \n",
      "25%        0.000259      0.081059                  0.000000  \n",
      "50%        0.000730      0.142789                  0.000000  \n",
      "75%        0.001412      0.374532                  0.000000  \n",
      "max     7100.000000    300.000000                  1.000000  \n",
      "\n",
      "[8 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "print(station_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Measurement date  Station code      Latitude  \\\n",
      "count                        3703662  3.703662e+06  3.703662e+06   \n",
      "mean   2022-06-04 01:49:42.133468160  2.159581e+02  3.755426e+01   \n",
      "min              2021-01-01 00:00:00  2.040000e+02  3.745236e+01   \n",
      "25%              2021-09-15 04:00:00  2.100000e+02  3.751753e+01   \n",
      "50%              2022-05-30 09:00:00  2.160000e+02  3.754496e+01   \n",
      "75%              2023-02-11 15:00:00  2.220000e+02  3.758485e+01   \n",
      "max              2023-12-31 23:00:00  2.280000e+02  3.765877e+01   \n",
      "std                              NaN  7.176528e+00  5.342886e-02   \n",
      "\n",
      "          Longitude           SO2           NO2            O3            CO  \\\n",
      "count  3.703662e+06  3.703662e+06  3.703662e+06  3.703662e+06  3.703662e+06   \n",
      "mean   1.269889e+02 -7.829759e-04  2.350889e-02  1.933672e-02  5.095291e-01   \n",
      "min    1.268352e+02 -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
      "25%    1.269271e+02  3.000000e-03  1.600000e-02  9.000000e-03  3.000000e-01   \n",
      "50%    1.270049e+02  4.000000e-03  2.500000e-02  2.100000e-02  5.000000e-01   \n",
      "75%    1.270475e+02  5.000000e-03  3.800000e-02  3.500000e-02  6.000000e-01   \n",
      "max    1.271368e+02  3.736000e+00  3.844500e+01  3.360000e+01  7.170000e+01   \n",
      "std    7.891744e-02  7.245694e-02  1.121034e-01  9.489902e-02  3.977503e-01   \n",
      "\n",
      "               PM10         PM2.5  ...  SO2_rolling_mean  NO2_rolling_mean  \\\n",
      "count  3.703662e+06  3.703662e+06  ...      3.703660e+06      3.703660e+06   \n",
      "mean   4.410450e+01  2.557335e+01  ...     -7.829785e-04      2.350887e-02   \n",
      "min   -1.000000e+00 -1.000000e+00  ...     -1.000000e+00     -1.000000e+00   \n",
      "25%    2.200000e+01  1.100000e+01  ...      3.000000e-03      1.600000e-02   \n",
      "50%    3.500000e+01  1.900000e+01  ...      4.000000e-03      2.500000e-02   \n",
      "75%    5.300000e+01  3.100000e+01  ...      5.000000e-03      3.800000e-02   \n",
      "max    3.586000e+03  6.256000e+03  ...      3.736000e+00      3.844500e+01   \n",
      "std    7.184132e+01  4.381902e+01  ...      7.207094e-02      1.095294e-01   \n",
      "\n",
      "       O3_rolling_mean  CO_rolling_mean  PM10_rolling_mean  \\\n",
      "count     3.703660e+06     3.703660e+06       3.703660e+06   \n",
      "mean      1.933672e-02     5.095289e-01       4.410449e+01   \n",
      "min      -1.000000e+00    -1.000000e+00      -1.000000e+00   \n",
      "25%       9.000000e-03     3.333333e-01       2.200000e+01   \n",
      "50%       2.100000e-02     5.000000e-01       3.500000e+01   \n",
      "75%       3.500000e-02     6.000000e-01       5.300000e+01   \n",
      "max       3.360000e+01     7.170000e+01       3.586000e+03   \n",
      "std       9.318317e-02     3.886905e-01       6.994562e+01   \n",
      "\n",
      "       PM2.5_rolling_mean       SO2_NO2       O3_PM10        SO2_O3  \\\n",
      "count        3.703660e+06  3.703662e+06  3.703662e+06  3.703662e+06   \n",
      "mean         2.557335e+01  7.860779e-01  9.469410e+00  6.135782e-01   \n",
      "min         -1.000000e+00 -1.000000e+05 -3.333322e-01 -1.000000e+05   \n",
      "25%          1.133333e+01  1.071046e-01  2.222222e-04  1.052078e-01   \n",
      "50%          1.900000e+01  1.621183e-01  6.249992e-04  1.903855e-01   \n",
      "75%          3.100000e+01  2.498751e-01  1.250000e-03  4.987531e-01   \n",
      "max          6.256000e+03  3.320000e+04  2.770000e+04  2.230000e+04   \n",
      "std          4.167132e+01  1.448702e+02  2.031336e+02  2.233420e+02   \n",
      "\n",
      "       instrument_status_binary  \n",
      "count              3.703662e+06  \n",
      "mean               2.618111e-02  \n",
      "min                0.000000e+00  \n",
      "25%                0.000000e+00  \n",
      "50%                0.000000e+00  \n",
      "75%                0.000000e+00  \n",
      "max                1.000000e+00  \n",
      "std                1.596736e-01  \n",
      "\n",
      "[8 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrument status\n",
      "0    97.381889\n",
      "8     0.764730\n",
      "1     0.732977\n",
      "9     0.531042\n",
      "4     0.484925\n",
      "2     0.104437\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Procesando estación 205 con contaminante SO2\n",
      "Clasificación de anomalías - F1 Score: 0.8180\n",
      "\n",
      "Procesando estación 209 con contaminante NO2\n",
      "Clasificación de anomalías - F1 Score: 0.7108\n",
      "\n",
      "Procesando estación 223 con contaminante O3\n",
      "Clasificación de anomalías - F1 Score: 0.6293\n",
      "\n",
      "Procesando estación 224 con contaminante CO\n",
      "Clasificación de anomalías - F1 Score: 0.5358\n",
      "\n",
      "Procesando estación 226 con contaminante PM10\n",
      "Clasificación de anomalías - F1 Score: 0.5519\n",
      "\n",
      "Procesando estación 227 con contaminante PM2.5\n",
      "Clasificación de anomalías - F1 Score: 0.5855\n",
      "Predicciones guardadas en predictions/predictions_task_3.json\n",
      "Instrument status\n",
      "0    97.381889\n",
      "8     0.764730\n",
      "1     0.732977\n",
      "9     0.531042\n",
      "4     0.484925\n",
      "2     0.104437\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv(\"data/raw/measurement_data.csv\")\n",
    "instrument_data = pd.read_csv(\"data/raw/instrument_data.csv\", parse_dates=['Measurement date'])\n",
    "\n",
    "print(instrument_data[\"Instrument status\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Convertir fechas a tipo datetime\n",
    "data['Measurement date'] = pd.to_datetime(data['Measurement date'])\n",
    "instrument_data['Measurement date'] = pd.to_datetime(instrument_data['Measurement date'])\n",
    "\n",
    "# Realizar el merge entre data e instrument_data\n",
    "merged_data = pd.merge(data, instrument_data, on=['Station code', 'Measurement date'], how='inner')\n",
    "\n",
    "# Extraer características temporales\n",
    "merged_data[\"hour\"] = merged_data[\"Measurement date\"].dt.hour\n",
    "merged_data[\"month\"] = merged_data[\"Measurement date\"].dt.month\n",
    "merged_data[\"weekday\"] = merged_data[\"Measurement date\"].dt.weekday\n",
    "merged_data[\"is_weekend\"] = merged_data[\"weekday\"].isin([5, 6]).astype(int)\n",
    "\n",
    "# Crear nuevas características\n",
    "# Promedio móvil de 3 horas para los contaminantes\n",
    "for pollutant in ['SO2', 'NO2', 'O3', 'CO', 'PM10', 'PM2.5']:\n",
    "    merged_data[f'{pollutant}_rolling_mean'] = merged_data[pollutant].rolling(window=3).mean()\n",
    "\n",
    "# Relación entre contaminantes\n",
    "merged_data['SO2_NO2'] = merged_data['SO2'] / (merged_data['NO2'] + 1e-5)  # Agregar un pequeño valor para evitar división por 0\n",
    "merged_data['O3_PM10'] = merged_data['O3'] / (merged_data['PM10'] + 1e-5)\n",
    "merged_data['SO2_O3'] = merged_data['SO2'] / (merged_data['O3'] + 1e-5)\n",
    "\n",
    "# Eliminar valores negativos para los contaminantes de todo el conjunto de datos\n",
    "contaminants = ['SO2', 'NO2', 'O3', 'CO', 'PM10', 'PM2.5', \n",
    "                'SO2_rolling_mean', 'NO2_rolling_mean', 'O3_rolling_mean', \n",
    "                'CO_rolling_mean', 'PM10_rolling_mean', 'PM2.5_rolling_mean', \n",
    "                'SO2_NO2', 'O3_PM10', 'SO2_O3']\n",
    "merged_data = merged_data[(merged_data[contaminants] >= 0).all(axis=1)]\n",
    "\n",
    "# Clasificar el `Instrument status` en binario (0 -> Normal, 1 -> Anómalo)\n",
    "merged_data['instrument_status_binary'] = np.where(merged_data['Instrument status'] == 0, 0, 1)\n",
    "\n",
    "# Definir las estaciones y contaminantes con sus períodos\n",
    "stations = {\n",
    "    \"205\": (\"SO2\", '2023-11-01 00:00:00', '2023-11-30 23:00:00'),\n",
    "    \"209\": (\"NO2\", '2023-09-01 00:00:00', '2023-09-30 23:00:00'),\n",
    "    \"223\": (\"O3\", '2023-07-01 00:00:00', '2023-07-31 23:00:00'),\n",
    "    \"224\": (\"CO\", '2023-10-01 00:00:00', '2023-10-31 23:00:00'),\n",
    "    \"226\": (\"PM10\", '2023-08-01 00:00:00', '2023-08-31 23:00:00'),\n",
    "    \"227\": (\"PM2.5\", '2023-12-01 00:00:00', '2023-12-31 23:00:00')\n",
    "}\n",
    "\n",
    "# Inicializar el diccionario de resultados\n",
    "output = {\"target\": {}}\n",
    "\n",
    "for station_code, (pollutant, start_date, end_date) in stations.items():\n",
    "    print(f\"\\nProcesando estación {station_code} con contaminante {pollutant}\")\n",
    "    station_data = merged_data[(merged_data['Station code'] == int(station_code)) & (merged_data[pollutant] >= 0)].copy()\n",
    "    \n",
    "    if station_data.empty:\n",
    "        continue\n",
    "    \n",
    "    # Aplicar Isolation Forest para detectar anomalías\n",
    "    features = ['hour', 'weekday', 'month', pollutant]\n",
    "    iso_forest = IsolationForest(contamination=0.04, random_state=42)\n",
    "    station_data['anomaly'] = iso_forest.fit_predict(station_data[features])\n",
    "    station_data['anomaly'] = np.where(station_data['anomaly'] == -1, 1, 0)\n",
    "    \n",
    "    # Filtrar anomalías detectadas\n",
    "    anomalies = station_data[station_data['anomaly'] == 1].copy()\n",
    "    if anomalies.empty:\n",
    "        continue\n",
    "    \n",
    "    # Entrenar modelo multiclase para clasificar anomalías\n",
    "    X_anomalies = anomalies[features]\n",
    "    y_anomalies = anomalies['Instrument status']\n",
    "    if y_anomalies.isnull().all():\n",
    "        continue\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_anomalies, y_anomalies, test_size=0.2, random_state=42)\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Clasificación de anomalías - F1 Score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "    \n",
    "    # Predecir anomalías en el período solicitado\n",
    "    date_range = pd.date_range(start=pd.to_datetime(start_date), end=pd.to_datetime(end_date), freq='h')\n",
    "    forecast_data = pd.DataFrame({'Measurement date': date_range})\n",
    "    forecast_data['hour'] = forecast_data['Measurement date'].dt.hour\n",
    "    forecast_data['weekday'] = forecast_data['Measurement date'].dt.weekday\n",
    "    forecast_data['month'] = forecast_data['Measurement date'].dt.month\n",
    "    forecast_data[pollutant] = 0  # Se podría usar un valor estimado o promedio\n",
    "    \n",
    "    # Aplicar Isolation Forest en los datos futuros\n",
    "    forecast_data['anomaly'] = iso_forest.predict(forecast_data[features])\n",
    "    forecast_data['anomaly'] = np.where(forecast_data['anomaly'] == -1, 1, 0)\n",
    "    \n",
    "    anomalous_data = forecast_data[forecast_data['anomaly'] == 1].copy()\n",
    "    if not anomalous_data.empty:\n",
    "        anomalous_data['anomaly_type'] = clf.predict(anomalous_data[features])\n",
    "    \n",
    "    final_results = forecast_data.merge(anomalous_data[['Measurement date', 'anomaly_type']], on='Measurement date', how='left')\n",
    "    output[\"target\"][station_code] = {str(date): int(pred) if not np.isnan(pred) else 0 for date, pred in zip(final_results['Measurement date'], final_results['anomaly_type'].fillna(0))}\n",
    "\n",
    "\n",
    "# Guardar predicciones\n",
    "output_filename = \"predictions/predictions_task_3.json\"\n",
    "os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "print(f\"Predicciones guardadas en {output_filename}\")\n",
    "print(instrument_data[\"Instrument status\"].value_counts(normalize=True) * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Measurement date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Station code",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SO2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "NO2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "O3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CO",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PM10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PM2.5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Item code",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Average value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Instrument status",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hour",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "month",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "weekday",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "is_weekend",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SO2_rolling_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "NO2_rolling_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "O3_rolling_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CO_rolling_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PM10_rolling_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PM2.5_rolling_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SO2_NO2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "O3_PM10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SO2_O3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "instrument_status_binary",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "anomaly",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4ea946f8-4185-4d85-ac7e-560da9dc1dbd",
       "rows": [
        [
         "count",
         "150665",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0",
         "150665.0"
        ],
        [
         "mean",
         "2022-06-12 16:07:59.816812032",
         "227.0",
         "37.50268570000002",
         "127.09250919999997",
         "0.0040327813360767255",
         "0.029552464075930044",
         "0.023859788271994163",
         "0.5644861115720307",
         "46.15344638768128",
         "24.097082932333322",
         "4.333634221617496",
         "11.812449613380677",
         "0.12010752331331098",
         "11.506116218099757",
         "6.393017621876348",
         "3.000159293797498",
         "0.28648989479972126",
         "0.004032794610559846",
         "0.029551855662121172",
         "0.023859640040267555",
         "0.5644721733647496",
         "46.15103043175257",
         "24.091018705959137",
         "0.509060518132863",
         "11.521873382420667",
         "0.8582767825936537",
         "0.02954236219427206",
         "0.03996946868881293"
        ],
        [
         "min",
         "2021-01-01 00:00:00",
         "227.0",
         "37.5026857",
         "127.0925092",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "25%",
         "2021-09-20 21:00:00",
         "227.0",
         "37.5026857",
         "127.0925092",
         "0.003",
         "0.017",
         "0.006",
         "0.4",
         "22.0",
         "11.0",
         "2.0",
         "0.011",
         "0.0",
         "6.0",
         "4.0",
         "1.0",
         "0.0",
         "0.003",
         "0.017",
         "0.006",
         "0.4",
         "22.0",
         "10.666666666666666",
         "0.09801999607920016",
         "0.00015624995117189023",
         "0.09995002498750626",
         "0.0",
         "0.0"
        ],
        [
         "50%",
         "2022-06-10 01:00:00",
         "227.0",
         "37.5026857",
         "127.0925092",
         "0.004",
         "0.026",
         "0.02",
         "0.5",
         "35.0",
         "18.0",
         "5.0",
         "0.069",
         "0.0",
         "12.0",
         "6.0",
         "3.0",
         "0.0",
         "0.004",
         "0.026333333333336827",
         "0.02",
         "0.5",
         "35.333333333333336",
         "18.333333333333332",
         "0.13950244129272263",
         "0.0005833331712963413",
         "0.18173557473875512",
         "0.0",
         "0.0"
        ],
        [
         "75%",
         "2023-02-26 17:00:00",
         "227.0",
         "37.5026857",
         "127.0925092",
         "0.005",
         "0.04",
         "0.035",
         "0.7",
         "54.0",
         "30.0",
         "7.0",
         "14.0",
         "0.0",
         "18.0",
         "9.0",
         "5.0",
         "1.0",
         "0.005",
         "0.04",
         "0.035",
         "0.7",
         "54.0",
         "30.0",
         "0.21413276231263384",
         "0.0012631575623269571",
         "0.6242197253433209",
         "0.0",
         "0.0"
        ],
        [
         "max",
         "2023-11-30 23:00:00",
         "227.0",
         "37.5026857",
         "127.0925092",
         "0.124",
         "0.114",
         "0.17",
         "12.4",
         "985.0",
         "985.0",
         "8.0",
         "985.0",
         "9.0",
         "23.0",
         "12.0",
         "6.0",
         "1.0",
         "0.124",
         "0.114",
         "0.17",
         "12.4",
         "985.0",
         "985.0",
         "1400.0",
         "4299.999999999999",
         "499.99999999999994",
         "1.0",
         "1.0"
        ],
        [
         "std",
         null,
         "0.0",
         "2.1316352813811817e-14",
         "2.8421803751749087e-14",
         "0.0027011017280842145",
         "0.015930537609836506",
         "0.020422736428933115",
         "0.37495717234676157",
         "71.31421593311507",
         "40.32766446184223",
         "2.7486911882144565",
         "37.8289962962009",
         "0.833463771594336",
         "6.91825031898522",
         "3.3734113281066933",
         "2.000783040071041",
         "0.45212254061008456",
         "0.0025699694066176346",
         "0.01585471545485159",
         "0.02033885574532641",
         "0.36272731409415643",
         "68.82376109507577",
         "38.422740065721136",
         "13.69472233147398",
         "151.65634219754895",
         "11.399797073014705",
         "0.1693215914130605",
         "0.19588814396709914"
        ]
       ],
       "shape": {
        "columns": 28,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Measurement date</th>\n",
       "      <th>Station code</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>...</th>\n",
       "      <th>NO2_rolling_mean</th>\n",
       "      <th>O3_rolling_mean</th>\n",
       "      <th>CO_rolling_mean</th>\n",
       "      <th>PM10_rolling_mean</th>\n",
       "      <th>PM2.5_rolling_mean</th>\n",
       "      <th>SO2_NO2</th>\n",
       "      <th>O3_PM10</th>\n",
       "      <th>SO2_O3</th>\n",
       "      <th>instrument_status_binary</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150665</td>\n",
       "      <td>150665.0</td>\n",
       "      <td>1.506650e+05</td>\n",
       "      <td>1.506650e+05</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "      <td>150665.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-06-12 16:07:59.816812032</td>\n",
       "      <td>227.0</td>\n",
       "      <td>3.750269e+01</td>\n",
       "      <td>1.270925e+02</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>0.023860</td>\n",
       "      <td>0.564486</td>\n",
       "      <td>46.153446</td>\n",
       "      <td>24.097083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>0.023860</td>\n",
       "      <td>0.564472</td>\n",
       "      <td>46.151030</td>\n",
       "      <td>24.091019</td>\n",
       "      <td>0.509061</td>\n",
       "      <td>11.521873</td>\n",
       "      <td>0.858277</td>\n",
       "      <td>0.029542</td>\n",
       "      <td>0.039969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>227.0</td>\n",
       "      <td>3.750269e+01</td>\n",
       "      <td>1.270925e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-09-20 21:00:00</td>\n",
       "      <td>227.0</td>\n",
       "      <td>3.750269e+01</td>\n",
       "      <td>1.270925e+02</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.098020</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.099950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-06-10 01:00:00</td>\n",
       "      <td>227.0</td>\n",
       "      <td>3.750269e+01</td>\n",
       "      <td>1.270925e+02</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026333</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>18.333333</td>\n",
       "      <td>0.139502</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.181736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-02-26 17:00:00</td>\n",
       "      <td>227.0</td>\n",
       "      <td>3.750269e+01</td>\n",
       "      <td>1.270925e+02</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.214133</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.624220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-11-30 23:00:00</td>\n",
       "      <td>227.0</td>\n",
       "      <td>3.750269e+01</td>\n",
       "      <td>1.270925e+02</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>4300.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.131635e-14</td>\n",
       "      <td>2.842180e-14</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.015931</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>0.374957</td>\n",
       "      <td>71.314216</td>\n",
       "      <td>40.327664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015855</td>\n",
       "      <td>0.020339</td>\n",
       "      <td>0.362727</td>\n",
       "      <td>68.823761</td>\n",
       "      <td>38.422740</td>\n",
       "      <td>13.694722</td>\n",
       "      <td>151.656342</td>\n",
       "      <td>11.399797</td>\n",
       "      <td>0.169322</td>\n",
       "      <td>0.195888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Measurement date  Station code      Latitude  \\\n",
       "count                         150665      150665.0  1.506650e+05   \n",
       "mean   2022-06-12 16:07:59.816812032         227.0  3.750269e+01   \n",
       "min              2021-01-01 00:00:00         227.0  3.750269e+01   \n",
       "25%              2021-09-20 21:00:00         227.0  3.750269e+01   \n",
       "50%              2022-06-10 01:00:00         227.0  3.750269e+01   \n",
       "75%              2023-02-26 17:00:00         227.0  3.750269e+01   \n",
       "max              2023-11-30 23:00:00         227.0  3.750269e+01   \n",
       "std                              NaN           0.0  2.131635e-14   \n",
       "\n",
       "          Longitude            SO2            NO2             O3  \\\n",
       "count  1.506650e+05  150665.000000  150665.000000  150665.000000   \n",
       "mean   1.270925e+02       0.004033       0.029552       0.023860   \n",
       "min    1.270925e+02       0.000000       0.000000       0.000000   \n",
       "25%    1.270925e+02       0.003000       0.017000       0.006000   \n",
       "50%    1.270925e+02       0.004000       0.026000       0.020000   \n",
       "75%    1.270925e+02       0.005000       0.040000       0.035000   \n",
       "max    1.270925e+02       0.124000       0.114000       0.170000   \n",
       "std    2.842180e-14       0.002701       0.015931       0.020423   \n",
       "\n",
       "                  CO           PM10          PM2.5  ...  NO2_rolling_mean  \\\n",
       "count  150665.000000  150665.000000  150665.000000  ...     150665.000000   \n",
       "mean        0.564486      46.153446      24.097083  ...          0.029552   \n",
       "min         0.000000       0.000000       0.000000  ...          0.000000   \n",
       "25%         0.400000      22.000000      11.000000  ...          0.017000   \n",
       "50%         0.500000      35.000000      18.000000  ...          0.026333   \n",
       "75%         0.700000      54.000000      30.000000  ...          0.040000   \n",
       "max        12.400000     985.000000     985.000000  ...          0.114000   \n",
       "std         0.374957      71.314216      40.327664  ...          0.015855   \n",
       "\n",
       "       O3_rolling_mean  CO_rolling_mean  PM10_rolling_mean  \\\n",
       "count    150665.000000    150665.000000      150665.000000   \n",
       "mean          0.023860         0.564472          46.151030   \n",
       "min           0.000000         0.000000           0.000000   \n",
       "25%           0.006000         0.400000          22.000000   \n",
       "50%           0.020000         0.500000          35.333333   \n",
       "75%           0.035000         0.700000          54.000000   \n",
       "max           0.170000        12.400000         985.000000   \n",
       "std           0.020339         0.362727          68.823761   \n",
       "\n",
       "       PM2.5_rolling_mean        SO2_NO2        O3_PM10         SO2_O3  \\\n",
       "count       150665.000000  150665.000000  150665.000000  150665.000000   \n",
       "mean            24.091019       0.509061      11.521873       0.858277   \n",
       "min              0.000000       0.000000       0.000000       0.000000   \n",
       "25%             10.666667       0.098020       0.000156       0.099950   \n",
       "50%             18.333333       0.139502       0.000583       0.181736   \n",
       "75%             30.000000       0.214133       0.001263       0.624220   \n",
       "max            985.000000    1400.000000    4300.000000     500.000000   \n",
       "std             38.422740      13.694722     151.656342      11.399797   \n",
       "\n",
       "       instrument_status_binary        anomaly  \n",
       "count             150665.000000  150665.000000  \n",
       "mean                   0.029542       0.039969  \n",
       "min                    0.000000       0.000000  \n",
       "25%                    0.000000       0.000000  \n",
       "50%                    0.000000       0.000000  \n",
       "75%                    0.000000       0.000000  \n",
       "max                    1.000000       1.000000  \n",
       "std                    0.169322       0.195888  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrument status\n",
      "0    97.381889\n",
      "8     0.764730\n",
      "1     0.732977\n",
      "9     0.531042\n",
      "4     0.484925\n",
      "2     0.104437\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Procesando estación 205 con contaminante SO2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jjaavviieerroperro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8971 - loss: 0.5006 - val_accuracy: 0.9830 - val_loss: 0.1061\n",
      "Epoch 2/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9697 - loss: 0.1578 - val_accuracy: 0.9830 - val_loss: 0.0888\n",
      "Epoch 3/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9773 - loss: 0.1137 - val_accuracy: 0.9830 - val_loss: 0.1115\n",
      "Epoch 4/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9738 - loss: 0.1181 - val_accuracy: 0.9830 - val_loss: 0.0902\n",
      "Epoch 5/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9741 - loss: 0.1156 - val_accuracy: 0.9830 - val_loss: 0.0819\n",
      "Epoch 6/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9725 - loss: 0.1223 - val_accuracy: 0.9830 - val_loss: 0.0787\n",
      "Epoch 7/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9785 - loss: 0.0946 - val_accuracy: 0.9830 - val_loss: 0.0813\n",
      "Epoch 8/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9797 - loss: 0.0937 - val_accuracy: 0.9830 - val_loss: 0.0800\n",
      "Epoch 9/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.1013 - val_accuracy: 0.9830 - val_loss: 0.0765\n",
      "Epoch 10/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9746 - loss: 0.1017 - val_accuracy: 0.9830 - val_loss: 0.0739\n",
      "Epoch 11/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9734 - loss: 0.1081 - val_accuracy: 0.9830 - val_loss: 0.0754\n",
      "Epoch 12/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9751 - loss: 0.1044 - val_accuracy: 0.9830 - val_loss: 0.0780\n",
      "Epoch 13/100\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9748 - loss: 0.0994 - val_accuracy: 0.9830 - val_loss: 0.1041\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Clasificación de anomalías - F1 Score: 0.4957\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "Procesando estación 209 con contaminante NO2\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jjaavviieerroperro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9229 - loss: 0.4534 - val_accuracy: 0.9879 - val_loss: 0.0936\n",
      "Epoch 2/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9893 - loss: 0.1184 - val_accuracy: 0.9879 - val_loss: 0.1034\n",
      "Epoch 3/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9887 - loss: 0.0952 - val_accuracy: 0.9879 - val_loss: 0.0649\n",
      "Epoch 4/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9900 - loss: 0.0714 - val_accuracy: 0.9879 - val_loss: 0.0659\n",
      "Epoch 5/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9881 - loss: 0.0763 - val_accuracy: 0.9879 - val_loss: 0.0747\n",
      "Epoch 6/100\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9874 - loss: 0.0768 - val_accuracy: 0.9879 - val_loss: 0.0696\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Clasificación de anomalías - F1 Score: 0.2485\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "\n",
      "Procesando estación 223 con contaminante O3\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jjaavviieerroperro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9882 - loss: 0.2837 - val_accuracy: 0.9794 - val_loss: 0.1254\n",
      "Epoch 2/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9904 - loss: 0.0642 - val_accuracy: 0.9794 - val_loss: 0.1067\n",
      "Epoch 3/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9860 - loss: 0.0868 - val_accuracy: 0.9794 - val_loss: 0.1079\n",
      "Epoch 4/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.0522 - val_accuracy: 0.9794 - val_loss: 0.1092\n",
      "Epoch 5/100\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9872 - loss: 0.0732 - val_accuracy: 0.9794 - val_loss: 0.1091\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Clasificación de anomalías - F1 Score: 0.2474\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\n",
      "Procesando estación 224 con contaminante CO\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jjaavviieerroperro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9046 - loss: 0.5750 - val_accuracy: 0.9641 - val_loss: 0.2249\n",
      "Epoch 2/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9615 - loss: 0.2251 - val_accuracy: 0.9653 - val_loss: 0.1930\n",
      "Epoch 3/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9570 - loss: 0.2050 - val_accuracy: 0.9641 - val_loss: 0.1718\n",
      "Epoch 4/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9611 - loss: 0.1699 - val_accuracy: 0.9677 - val_loss: 0.1723\n",
      "Epoch 5/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9628 - loss: 0.1594 - val_accuracy: 0.9653 - val_loss: 0.1611\n",
      "Epoch 6/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9672 - loss: 0.1431 - val_accuracy: 0.9653 - val_loss: 0.1383\n",
      "Epoch 7/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9576 - loss: 0.1578 - val_accuracy: 0.9653 - val_loss: 0.1353\n",
      "Epoch 8/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.1307 - val_accuracy: 0.9713 - val_loss: 0.1212\n",
      "Epoch 9/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9639 - loss: 0.1371 - val_accuracy: 0.9713 - val_loss: 0.1244\n",
      "Epoch 10/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9670 - loss: 0.1282 - val_accuracy: 0.9701 - val_loss: 0.1162\n",
      "Epoch 11/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9691 - loss: 0.1182 - val_accuracy: 0.9617 - val_loss: 0.1324\n",
      "Epoch 12/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.1232 - val_accuracy: 0.9713 - val_loss: 0.1224\n",
      "Epoch 13/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9690 - loss: 0.1136 - val_accuracy: 0.9725 - val_loss: 0.1151\n",
      "Epoch 14/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9726 - loss: 0.1063 - val_accuracy: 0.9713 - val_loss: 0.1220\n",
      "Epoch 15/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9718 - loss: 0.1152 - val_accuracy: 0.9737 - val_loss: 0.1289\n",
      "Epoch 16/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9692 - loss: 0.1125 - val_accuracy: 0.9665 - val_loss: 0.1132\n",
      "Epoch 17/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9705 - loss: 0.1128 - val_accuracy: 0.9725 - val_loss: 0.1195\n",
      "Epoch 18/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.0871 - val_accuracy: 0.9677 - val_loss: 0.1229\n",
      "Epoch 19/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9713 - loss: 0.1032 - val_accuracy: 0.9689 - val_loss: 0.1291\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Clasificación de anomalías - F1 Score: 0.2997\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\n",
      "Procesando estación 226 con contaminante PM10\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jjaavviieerroperro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8935 - loss: 0.5275 - val_accuracy: 0.9340 - val_loss: 0.4150\n",
      "Epoch 2/100\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9400 - loss: 0.3372 - val_accuracy: 0.7597 - val_loss: 0.4858\n",
      "Epoch 3/100\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9267 - loss: 0.3040 - val_accuracy: 0.8780 - val_loss: 0.3703\n",
      "Epoch 4/100\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9359 - loss: 0.2573 - val_accuracy: 0.9352 - val_loss: 0.2122\n",
      "Epoch 5/100\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9505 - loss: 0.2058 - val_accuracy: 0.9066 - val_loss: 0.2405\n",
      "Epoch 6/100\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9358 - loss: 0.2230 - val_accuracy: 0.9390 - val_loss: 0.1970\n",
      "Epoch 7/100\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9518 - loss: 0.1702 - val_accuracy: 0.9402 - val_loss: 0.1957\n",
      "Epoch 8/100\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9493 - loss: 0.1815 - val_accuracy: 0.9390 - val_loss: 0.1869\n",
      "Epoch 9/100\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9503 - loss: 0.1546 - val_accuracy: 0.9427 - val_loss: 0.2003\n",
      "Epoch 10/100\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9513 - loss: 0.1593 - val_accuracy: 0.9415 - val_loss: 0.1920\n",
      "Epoch 11/100\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9505 - loss: 0.1637 - val_accuracy: 0.9253 - val_loss: 0.1873\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Clasificación de anomalías - F1 Score: 0.4994\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\n",
      "Procesando estación 227 con contaminante PM2.5\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jjaavviieerroperro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9055 - loss: 0.8518 - val_accuracy: 0.9480 - val_loss: 0.7321\n",
      "Epoch 2/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.2959 - val_accuracy: 0.9546 - val_loss: 0.2765\n",
      "Epoch 3/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9617 - loss: 0.2260 - val_accuracy: 0.9546 - val_loss: 0.2283\n",
      "Epoch 4/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9629 - loss: 0.1915 - val_accuracy: 0.9291 - val_loss: 0.2280\n",
      "Epoch 5/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9611 - loss: 0.1940 - val_accuracy: 0.9480 - val_loss: 0.2097\n",
      "Epoch 6/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9633 - loss: 0.1621 - val_accuracy: 0.9247 - val_loss: 0.2505\n",
      "Epoch 7/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9622 - loss: 0.1656 - val_accuracy: 0.9480 - val_loss: 0.2200\n",
      "Epoch 8/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9635 - loss: 0.1537 - val_accuracy: 0.9380 - val_loss: 0.1957\n",
      "Epoch 9/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9571 - loss: 0.1579 - val_accuracy: 0.9557 - val_loss: 0.1851\n",
      "Epoch 10/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9620 - loss: 0.1418 - val_accuracy: 0.9457 - val_loss: 0.1850\n",
      "Epoch 11/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9602 - loss: 0.1484 - val_accuracy: 0.9480 - val_loss: 0.1936\n",
      "Epoch 12/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9640 - loss: 0.1454 - val_accuracy: 0.9302 - val_loss: 0.1944\n",
      "Epoch 13/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9648 - loss: 0.1262 - val_accuracy: 0.9313 - val_loss: 0.1858\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Clasificación de anomalías - F1 Score: 0.4553\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Predicciones guardadas en predictions/predictions_task_3.json\n",
      "Instrument status\n",
      "0    97.381889\n",
      "8     0.764730\n",
      "1     0.732977\n",
      "9     0.531042\n",
      "4     0.484925\n",
      "2     0.104437\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv(\"data/raw/measurement_data.csv\")\n",
    "instrument_data = pd.read_csv(\"data/raw/instrument_data.csv\", parse_dates=['Measurement date'])\n",
    "\n",
    "print(instrument_data[\"Instrument status\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Convertir fechas a tipo datetime\n",
    "data['Measurement date'] = pd.to_datetime(data['Measurement date'])\n",
    "instrument_data['Measurement date'] = pd.to_datetime(instrument_data['Measurement date'])\n",
    "\n",
    "# Realizar el merge entre data e instrument_data\n",
    "merged_data = pd.merge(data, instrument_data, on=['Station code', 'Measurement date'], how='inner')\n",
    "\n",
    "# Extraer características temporales\n",
    "merged_data[\"hour\"] = merged_data[\"Measurement date\"].dt.hour\n",
    "merged_data[\"month\"] = merged_data[\"Measurement date\"].dt.month\n",
    "merged_data[\"weekday\"] = merged_data[\"Measurement date\"].dt.weekday\n",
    "merged_data[\"is_weekend\"] = merged_data[\"weekday\"].isin([5, 6]).astype(int)\n",
    "\n",
    "# Crear nuevas características\n",
    "# Promedio móvil de 3 horas para los contaminantes\n",
    "for pollutant in ['SO2', 'NO2', 'O3', 'CO', 'PM10', 'PM2.5']:\n",
    "    merged_data[f'{pollutant}_rolling_mean'] = merged_data[pollutant].rolling(window=3).mean()\n",
    "\n",
    "# Relación entre contaminantes\n",
    "merged_data['SO2_NO2'] = merged_data['SO2'] / (merged_data['NO2'] + 1e-5)  # Agregar un pequeño valor para evitar división por 0\n",
    "merged_data['O3_PM10'] = merged_data['O3'] / (merged_data['PM10'] + 1e-5)\n",
    "merged_data['SO2_O3'] = merged_data['SO2'] / (merged_data['O3'] + 1e-5)\n",
    "\n",
    "# Clasificar el `Instrument status` en las categorías proporcionadas\n",
    "# 0 -> Normal, 1 -> Need for calibration, 2 -> Abnormal, 4 -> Power cut off, 8 -> Under repair, 9 -> Abnormal data\n",
    "valid_statuses = [0, 1, 2, 4, 8, 9]\n",
    "merged_data = merged_data[merged_data['Instrument status'].isin(valid_statuses)]\n",
    "\n",
    "# Mapeo de Instrument status a clases 0-5\n",
    "status_map = {0: 0, 1: 1, 2: 2, 4: 3, 8: 4, 9: 5}\n",
    "merged_data['instrument_status_mapped'] = merged_data['Instrument status'].map(status_map)\n",
    "\n",
    "# Definir las estaciones y contaminantes con sus períodos\n",
    "stations = {\n",
    "    \"205\": (\"SO2\", '2023-11-01 00:00:00', '2023-11-30 23:00:00'),\n",
    "    \"209\": (\"NO2\", '2023-09-01 00:00:00', '2023-09-30 23:00:00'),\n",
    "    \"223\": (\"O3\", '2023-07-01 00:00:00', '2023-07-31 23:00:00'),\n",
    "    \"224\": (\"CO\", '2023-10-01 00:00:00', '2023-10-31 23:00:00'),\n",
    "    \"226\": (\"PM10\", '2023-08-01 00:00:00', '2023-08-31 23:00:00'),\n",
    "    \"227\": (\"PM2.5\", '2023-12-01 00:00:00', '2023-12-31 23:00:00')\n",
    "}\n",
    "\n",
    "# Inicializar el diccionario de resultados\n",
    "output = {\"target\": {}}\n",
    "\n",
    "# Mapeo para las predicciones de clase (0-5) a los valores originales (0, 1, 2, 4, 8, 9)\n",
    "reverse_status_map = {0: 0, 1: 1, 2: 2, 3: 4, 4: 8, 5: 9}\n",
    "\n",
    "for station_code, (pollutant, start_date, end_date) in stations.items():\n",
    "    print(f\"\\nProcesando estación {station_code} con contaminante {pollutant}\")\n",
    "    station_data = merged_data[(merged_data['Station code'] == int(station_code)) & (merged_data[pollutant] >= 0)].copy()\n",
    "\n",
    "    if station_data.empty:\n",
    "        continue\n",
    "\n",
    "    # Aplicar Isolation Forest para detectar anomalías\n",
    "    features = ['hour', 'weekday', 'month', pollutant]\n",
    "    iso_forest = IsolationForest(contamination=0.03, random_state=42)\n",
    "    station_data['anomaly'] = iso_forest.fit_predict(station_data[features])\n",
    "    station_data['anomaly'] = np.where(station_data['anomaly'] == -1, 1, 0)\n",
    "\n",
    "    # Filtrar anomalías detectadas\n",
    "    anomalies = station_data[station_data['anomaly'] == 1].copy()\n",
    "    if anomalies.empty:\n",
    "        continue\n",
    "\n",
    "    # Entrenar la red neuronal para clasificar anomalías\n",
    "    X_anomalies = anomalies[features]\n",
    "    y_anomalies = anomalies['instrument_status_mapped']\n",
    "    if y_anomalies.isnull().all():\n",
    "        continue\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_anomalies, y_anomalies, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Red neuronal más compleja\n",
    "    model = Sequential([\n",
    "        Dense(256, input_dim=X_train.shape[1], activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(6, activation='softmax')  # Salida multiclase con 6 clases posibles (0, 1, 2, 4, 8, 9)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # EarlyStopping para evitar el overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(f\"Clasificación de anomalías - F1 Score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "    # Predecir anomalías en el período solicitado\n",
    "    date_range = pd.date_range(start=pd.to_datetime(start_date), end=pd.to_datetime(end_date), freq='h')\n",
    "    forecast_data = pd.DataFrame({'Measurement date': date_range})\n",
    "    forecast_data['hour'] = forecast_data['Measurement date'].dt.hour\n",
    "    forecast_data['weekday'] = forecast_data['Measurement date'].dt.weekday\n",
    "    forecast_data['month'] = forecast_data['Measurement date'].dt.month\n",
    "    forecast_data[pollutant] = 0  # Se podría usar un valor estimado o promedio\n",
    "\n",
    "    # Aplicar Isolation Forest en los datos futuros\n",
    "    forecast_data['anomaly'] = iso_forest.predict(forecast_data[features])\n",
    "    forecast_data['anomaly'] = np.where(forecast_data['anomaly'] == -1, 1, 0)\n",
    "\n",
    "    anomalous_data = forecast_data[forecast_data['anomaly'] == 1].copy()\n",
    "    if not anomalous_data.empty:\n",
    "        # Convertir la predicción de clases (0-5) a los valores originales (0, 1, 2, 4, 8, 9)\n",
    "        anomalous_data['anomaly_type'] = [reverse_status_map[label] for label in np.argmax(model.predict(anomalous_data[features]), axis=1)]\n",
    "\n",
    "    # Merge de las predicciones de anomalías con los datos de forecast\n",
    "    final_results = forecast_data.merge(anomalous_data[['Measurement date', 'anomaly_type']], on='Measurement date', how='left')\n",
    "    output[\"target\"][station_code] = {str(date): int(pred) if not np.isnan(pred) else 0 for date, pred in zip(final_results['Measurement date'], final_results['anomaly_type'].fillna(0))}\n",
    "\n",
    "# Guardar predicciones\n",
    "output_filename = \"predictions/predictions_task_3.json\"\n",
    "os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "print(f\"Predicciones guardadas en {output_filename}\")\n",
    "print(instrument_data[\"Instrument status\"].value_counts(normalize=True) * 100)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
